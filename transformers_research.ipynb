{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mO0Ak7zixKRo",
        "outputId": "5002c354-d576-44a5-afaf-325d4c23ef9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing training size: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='44' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 44/500 00:01 < 00:17, 26.68 it/s, Epoch 44/500]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Sequence Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.527300</td>\n",
              "      <td>2.409162</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.460300</td>\n",
              "      <td>2.335792</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.382600</td>\n",
              "      <td>2.265625</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.284800</td>\n",
              "      <td>2.198680</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.224400</td>\n",
              "      <td>2.134938</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.118700</td>\n",
              "      <td>2.073853</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.089700</td>\n",
              "      <td>2.015904</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.032400</td>\n",
              "      <td>1.960835</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.016000</td>\n",
              "      <td>1.908605</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.903400</td>\n",
              "      <td>1.858921</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.870200</td>\n",
              "      <td>1.811488</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.793300</td>\n",
              "      <td>1.765703</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.767800</td>\n",
              "      <td>1.721496</td>\n",
              "      <td>0.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.759300</td>\n",
              "      <td>1.678878</td>\n",
              "      <td>0.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.712300</td>\n",
              "      <td>1.637590</td>\n",
              "      <td>0.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.738200</td>\n",
              "      <td>1.597856</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.606100</td>\n",
              "      <td>1.559155</td>\n",
              "      <td>0.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.607000</td>\n",
              "      <td>1.521381</td>\n",
              "      <td>0.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.544600</td>\n",
              "      <td>1.484342</td>\n",
              "      <td>0.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.484700</td>\n",
              "      <td>1.447713</td>\n",
              "      <td>0.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.476600</td>\n",
              "      <td>1.411843</td>\n",
              "      <td>0.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.447500</td>\n",
              "      <td>1.376532</td>\n",
              "      <td>0.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.432500</td>\n",
              "      <td>1.341672</td>\n",
              "      <td>0.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.384600</td>\n",
              "      <td>1.307474</td>\n",
              "      <td>0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.316400</td>\n",
              "      <td>1.273862</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.308200</td>\n",
              "      <td>1.240859</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.282900</td>\n",
              "      <td>1.208403</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.207100</td>\n",
              "      <td>1.176190</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.221400</td>\n",
              "      <td>1.144489</td>\n",
              "      <td>0.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.197200</td>\n",
              "      <td>1.113427</td>\n",
              "      <td>0.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.154900</td>\n",
              "      <td>1.082981</td>\n",
              "      <td>0.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.152500</td>\n",
              "      <td>1.053139</td>\n",
              "      <td>0.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.073400</td>\n",
              "      <td>1.023715</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.120400</td>\n",
              "      <td>0.994584</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.985400</td>\n",
              "      <td>0.965849</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.021800</td>\n",
              "      <td>0.937565</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.970500</td>\n",
              "      <td>0.909974</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.003300</td>\n",
              "      <td>0.883058</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.930900</td>\n",
              "      <td>0.856610</td>\n",
              "      <td>0.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.888000</td>\n",
              "      <td>0.830838</td>\n",
              "      <td>0.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.905300</td>\n",
              "      <td>0.805772</td>\n",
              "      <td>0.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.887000</td>\n",
              "      <td>0.781370</td>\n",
              "      <td>0.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.813800</td>\n",
              "      <td>0.757578</td>\n",
              "      <td>0.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.816800</td>\n",
              "      <td>0.734363</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Metrics: {'eval_loss': 0.7343631982803345, 'eval_sequence_accuracy': 1.0, 'eval_runtime': 0.0062, 'eval_samples_per_second': 321.378, 'eval_steps_per_second': 160.689, 'epoch': 44.0}\n",
            "\n",
            "Testing training size: 4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8008ff60d901>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;31m# repeat runs 5 times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                 \u001b[0msubset_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_subset_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m                 \u001b[0msubset_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0mcheck_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcheck_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-8008ff60d901>\u001b[0m in \u001b[0;36mfind_subset_size\u001b[0;34m(hidden_size, d_ff, num_layers, num_head, VOCAB_SIZE, SEQ_LEN)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0mmax_subset_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;31m# find the upper bound for which the model fails to memorize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mmax_subset_size\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0msuperset_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcan_memorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_subset_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0mmax_subset_size\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mlow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_subset_size\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-8008ff60d901>\u001b[0m in \u001b[0;36mcan_memorize\u001b[0;34m(subset_size, debug_flag)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerDecoderModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHFWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epoch_reached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# UNCOMMENT CODE BELOW TO USE MLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-8008ff60d901>\u001b[0m in \u001b[0;36mget_stats\u001b[0;34m(dataset, model, collator)\u001b[0m\n\u001b[1;32m    262\u001b[0m     )\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0mmax_epoch_reached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mtest_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2624\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msteps_skipped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2626\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2627\u001b[0m                         self._maybe_log_save_evaluate(\n\u001b[1;32m   2628\u001b[0m                             \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_step_end\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_step_end\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    557\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/notebook.py\u001b[0m in \u001b[0;36mon_step_end\u001b[0;34m(self, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mf\"{state.epoch:.2f}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         self.training_tracker.update(\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch}/{state.num_train_epochs}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/notebook.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, value, force_update, comment)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_calls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_value\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforce_update\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/notebook.py\u001b[0m in \u001b[0;36mupdate_bar\u001b[0;34m(self, value, comment)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"]\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mf\", {self.comment}]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/notebook.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml_code\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-3>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[1;32m    702\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_width\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback, TrainerState, TrainerControl\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import csv\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# UNCOMMENT CODE BELOW TO USE MLP\n",
        "# class MLP(nn.Module):\n",
        "#     def __init__(self, vocab_size, seq_len, num_layers, hidden_dim):\n",
        "#         super().__init__()\n",
        "#         self.seq_len = seq_len\n",
        "#         self.vocab_size = vocab_size\n",
        "#         input_dim = vocab_size * seq_len\n",
        "#         output_dim = input_dim\n",
        "\n",
        "#         layers = []\n",
        "\n",
        "#         prev_dim = input_dim\n",
        "#         for layer in range(num_layers):\n",
        "#             layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "#             layers.append(nn.ReLU())\n",
        "#             prev_dim = hidden_dim\n",
        "\n",
        "#         layers.append(nn.Linear(prev_dim, output_dim))\n",
        "\n",
        "#         self.model = nn.Sequential(*layers)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         logits_flat = self.model(x)\n",
        "#         batch_size = x.size(0)\n",
        "#         logits = logits_flat.view(batch_size, self.seq_len, self.vocab_size)\n",
        "\n",
        "#         return logits\n",
        "\n",
        "\n",
        "# # wrapper class to fit Hugging Face trainer\n",
        "# class HFWrapper(nn.Module):\n",
        "#     def __init__(self, model, vocab_size, seq_len):\n",
        "#         super().__init__()\n",
        "#         self.model = model\n",
        "#         self.vocab_size = vocab_size\n",
        "#         self.seq_len = seq_len\n",
        "#         self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#     def forward(self, inputs, labels=None):\n",
        "#         logits = self.model(inputs)\n",
        "#         logits = logits.view(-1, self.seq_len, self.vocab_size)\n",
        "\n",
        "#         output = {\"logits\": logits}\n",
        "\n",
        "#         logits_flat = logits.view(-1, self.vocab_size)\n",
        "#         labels_flat = labels.view(-1)\n",
        "\n",
        "#         loss = self.loss_fn(logits_flat, labels_flat)\n",
        "#         output[\"loss\"] = loss\n",
        "\n",
        "#         return output\n",
        "\n",
        "# class SeqDataset(Dataset):\n",
        "#     def __init__(self, data_x, data_y):\n",
        "#         self.data_x = torch.tensor(data_x, dtype=torch.long)\n",
        "#         self.data_y = torch.tensor(data_y, dtype=torch.long)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data_x)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             'inputs': self.data_x[idx],\n",
        "#             'labels': self.data_y[idx]\n",
        "#         }\n",
        "\n",
        "# def tokens_to_flat_onehot(token_ids, vocab_size):\n",
        "#     device = token_ids.device\n",
        "\n",
        "#     one_hot = torch.nn.functional.one_hot(token_ids, num_classes=vocab_size).to(device)\n",
        "#     one_hot_flat = one_hot.view(token_ids.size(0), -1).float()\n",
        "\n",
        "#     return one_hot_flat\n",
        "\n",
        "# class OneHotCollator:\n",
        "#     def __init__(self, vocab_size):\n",
        "#         self.vocab_size = vocab_size\n",
        "\n",
        "#     def __call__(self, batch):\n",
        "#         inputs = torch.stack([item[\"inputs\"] for item in batch])\n",
        "#         labels = torch.stack([item[\"labels\"] for item in batch])\n",
        "\n",
        "#         one_hot_inputs = F.one_hot(inputs, num_classes=self.vocab_size)\n",
        "#         one_hot_inputs = one_hot_inputs.view(inputs.size(0), -1).float()\n",
        "\n",
        "#         return {\n",
        "#             \"inputs\": one_hot_inputs,\n",
        "#             \"labels\": labels\n",
        "#         }\n",
        "\n",
        "class TransformerDecoderModel(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim, d_ff, num_layers, nhead):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.seq_len = vocab_size # assume seq_len = vocab_size for our test runs\n",
        "\n",
        "        # embed tokens in hidden dimension\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
        "        # include positional encoding\n",
        "        self.pos_encoder = PositionalEncoding(max_len=vocab_size, d_model=hidden_dim)\n",
        "\n",
        "        # define decoder portion of transformer\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=d_ff,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        seq_len = self.seq_len\n",
        "        # causal mask\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(x.device)\n",
        "        logits = self.output_layer(self.transformer_decoder(x, x, tgt_mask=tgt_mask))\n",
        "        return logits\n",
        "\n",
        "# positional encoding for tokens in sequence\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, seq_len, _ = x.size()\n",
        "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
        "        pos_emb = self.pos_embedding(positions)\n",
        "        return x + pos_emb\n",
        "\n",
        "# prepares model for proper Hugging Face trainer format\n",
        "class HFWrapper(nn.Module):\n",
        "    def __init__(self, model, vocab_size):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, inputs, labels=None):\n",
        "        logits = self.model(inputs)\n",
        "        output = {\"logits\": logits}\n",
        "\n",
        "        logits_flat = logits.view(-1, self.vocab_size)\n",
        "        labels_flat = labels.view(-1)\n",
        "        loss = self.loss_fn(logits_flat, labels_flat)\n",
        "        output[\"loss\"] = loss\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# prepares data into proper Hugging Face trainer format\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, data_x, data_y):\n",
        "        self.data_x = torch.tensor(data_x, dtype=torch.long)\n",
        "        self.data_y = torch.tensor(data_y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'inputs': self.data_x[idx],\n",
        "            'labels': self.data_y[idx]\n",
        "        }\n",
        "\n",
        "# prepares data into proper batches for training\n",
        "class TokenCollator:\n",
        "    def __call__(self, batch):\n",
        "        inputs = torch.stack([item[\"inputs\"] for item in batch])\n",
        "        labels = torch.stack([item[\"labels\"] for item in batch])\n",
        "        return {\n",
        "            \"inputs\": inputs,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "# generates training set given a fixed vocab size, sequence length, and subset size (dataset size)\n",
        "def generate_data(vocab_size, seq_len, subset_size):\n",
        "    # generate a sequence of random tokens of length seq_len\n",
        "    def generate_sequence(vocab_size, seq_len):\n",
        "        return tuple(random.randint(0, vocab_size - 1) for i in range(seq_len))\n",
        "\n",
        "    input_sequences = set(generate_sequence(vocab_size, seq_len) for i in range(subset_size))\n",
        "    # check there's no overlapping input sequences\n",
        "    while len(input_sequences) < subset_size:\n",
        "        input_sequences = set(generate_sequence(vocab_size, seq_len) for i in range(subset_size))\n",
        "\n",
        "    label_sequences = set(generate_sequence(vocab_size, seq_len) for i in range(subset_size))\n",
        "    # check there's no overlapping label sequences\n",
        "    while not input_sequences.isdisjoint(label_sequences):\n",
        "        label_sequences = set(generate_sequence(vocab_size, seq_len) for i in range(subset_size))\n",
        "\n",
        "    input_sequences = list(input_sequences)\n",
        "    label_sequences = list(label_sequences)\n",
        "    # randomize the index-wise mapping\n",
        "    random.shuffle(label_sequences)\n",
        "\n",
        "    return input_sequences, label_sequences\n",
        "\n",
        "# compute the per-sequence accuracy\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    accuracy = (preds.flatten() == labels.flatten()).mean().item()\n",
        "    return {\"sequence_accuracy\": accuracy}\n",
        "\n",
        "# stop training when accuracy is 1 (perfect accuracy)\n",
        "class StopOnPerfectAccuracyCallback(TrainerCallback):\n",
        "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics, **kwargs):\n",
        "        acc = metrics.get(\"eval_sequence_accuracy\")\n",
        "        if acc == 1.0:\n",
        "            control.should_training_stop = True\n",
        "        return control\n",
        "\n",
        "def get_stats(dataset, model, collator):\n",
        "\n",
        "    # training arguments - adjust as needed\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results_32\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        per_device_train_batch_size=32,\n",
        "        num_train_epochs=500,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        load_best_model_at_end=True,\n",
        "        greater_is_better=False,\n",
        "        report_to=\"none\",\n",
        "        weight_decay=0.01,\n",
        "        learning_rate=5e-4, # use this learning rate for transformer\n",
        "        # learning_rate=1e-2, # use this learning rate for MLP\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # training stops early if perfect accuracy is reached or validation loss plateaus\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "        eval_dataset=dataset,\n",
        "        data_collator=collator,\n",
        "        callbacks=[StopOnPerfectAccuracyCallback(), EarlyStoppingCallback(\n",
        "            early_stopping_patience=3,\n",
        "            early_stopping_threshold=0.0001\n",
        "        )],\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    max_epoch_reached = (trainer.state.epoch == training_args.num_train_epochs)\n",
        "    test_metrics = trainer.evaluate(dataset)\n",
        "    print(\"Test Metrics:\", test_metrics)\n",
        "    test_accuracy = test_metrics.get(\"eval_sequence_accuracy\")\n",
        "    return test_accuracy, max_epoch_reached\n",
        "\n",
        "def find_subset_size(hidden_size, d_ff, num_layers, num_head, VOCAB_SIZE, SEQ_LEN):\n",
        "    def can_memorize(subset_size, debug_flag):\n",
        "        print()\n",
        "        print(f\"Testing training size: {subset_size}\")\n",
        "\n",
        "        # create training set\n",
        "        input_sequences, label_sequences = generate_data(VOCAB_SIZE, SEQ_LEN, subset_size)\n",
        "        dataset = SeqDataset(input_sequences, label_sequences)\n",
        "        collator = TokenCollator()\n",
        "\n",
        "        # create model with input params and get its accuracy on given subset size\n",
        "        base_model = TransformerDecoderModel(VOCAB_SIZE, hidden_size, d_ff, num_layers, num_head)\n",
        "        model = HFWrapper(base_model, VOCAB_SIZE)\n",
        "        test_accuracy, max_epoch_reached = get_stats(dataset, model, collator)\n",
        "\n",
        "        # UNCOMMENT CODE BELOW TO USE MLP\n",
        "        # input_sequences, label_sequences = generate_data(VOCAB_SIZE, SEQ_LEN, subset_size)\n",
        "        # dataset = SeqDataset(input_sequences, label_sequences)\n",
        "        # collator = OneHotCollator(VOCAB_SIZE)\n",
        "        # base_model = MLP(VOCAB_SIZE, SEQ_LEN, num_layers, hidden_size)\n",
        "        # model = HFWrapper(base_model, VOCAB_SIZE, SEQ_LEN)\n",
        "\n",
        "        # delete model for space\n",
        "        del model\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # for debugging purposes to see if the maximum epoch was reached\n",
        "        if debug_flag:\n",
        "            return test_accuracy == 1.0, max_epoch_reached\n",
        "        else:\n",
        "            return test_accuracy == 1.0\n",
        "\n",
        "    superset_size = VOCAB_SIZE ** SEQ_LEN\n",
        "    max_subset_size = 2\n",
        "    # find the upper bound for which the model fails to memorize\n",
        "    while max_subset_size <= superset_size and can_memorize(max_subset_size, False):\n",
        "        max_subset_size *= 2\n",
        "    low = max_subset_size // 2\n",
        "    high = min(max_subset_size, superset_size)\n",
        "\n",
        "    # conduct binary search to find threshold\n",
        "    # check if max epoch was ever reached during search\n",
        "    check_flag = False\n",
        "    while low < high:\n",
        "        mid = (low + high) // 2\n",
        "        can_mem, max_epoch_reached = can_memorize(mid, True)\n",
        "        if can_mem:\n",
        "            low = mid + 1\n",
        "        else:\n",
        "            high = mid\n",
        "        check_flag = check_flag or max_epoch_reached\n",
        "\n",
        "    threshold = low\n",
        "    return threshold, check_flag\n",
        "\n",
        "\n",
        "# adjust parameters (paired index-wise) as fit\n",
        "BASE_NUM_LAYERS  = [1, 2, 3]\n",
        "BASE_NUM_HEADS   = [1, 2, 4, 8]\n",
        "BASE_DFF = [380, 128, 40]\n",
        "HIDDEN_SIZE = 32\n",
        "\n",
        "VOCAB_SIZE = 10\n",
        "SEQ_LEN = 10\n",
        "\n",
        "\n",
        "with open(f'transformer_results_{HIDDEN_SIZE}.csv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['hidden_size', 'd_ff', 'num_heads','num_layers', 'max_epoch_reached', 'subset1', 'subset2', 'subset3', 'subset4', 'subset5'])\n",
        "    for index in range(len(BASE_DFF)):\n",
        "        for num_head in BASE_NUM_HEADS:\n",
        "            hidden_size = HIDDEN_SIZE\n",
        "            num_layers = BASE_NUM_LAYERS[index]\n",
        "            d_ff = BASE_DFF[index]\n",
        "            check_flag = False\n",
        "            subset_sizes = []\n",
        "            # repeat runs 5 times\n",
        "            for i in range(5):\n",
        "                subset_size, check = find_subset_size(hidden_size, d_ff, num_layers, num_head, VOCAB_SIZE, SEQ_LEN)\n",
        "                subset_sizes.append(subset_size)\n",
        "                check_flag = check or check_flag\n",
        "            writer.writerow([hidden_size, d_ff, num_head, num_layers, check_flag, subset_sizes[0], subset_sizes[1], subset_sizes[2], subset_sizes[3], subset_sizes[4]])"
      ]
    }
  ]
}